{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Can Oversampling imbalanced data improve model's performance?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have encountered the phrase \"Imbalanced data\" since you started your journy with data science. When do we call data \"imbalanced\"?.\n",
    "\n",
    "Well, If your boss gave you a dataset to complete a particular classification task, you would first look for the target/dependent/class variable which is always categorical in such cases. If the target variable has two levels, you are then in charge of building a binary classification model. If the there are more than two classes in the target variable, the problem becomes a multiclass classification one. For the sake of simplicity, I will use a binary classification problem- The bank loan. The dataset consists of 614 examples/records/customers who have a pllied for a bank loan. 13 features/predictors/independent variables were extracted from each customer and the loan status was given as a target variable with two levels/classes: \"yes\" when the loan was granted and \"No\" when the application was rejected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "clear all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import import required packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into a dataframe\n",
    "df=pd.read_csv('dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape   # The dataset consists of 614 records/examples, which makes it a small  one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001006</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2583</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001008</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
       "0  LP001002   Male      No          0      Graduate            No   \n",
       "1  LP001003   Male     Yes          1      Graduate            No   \n",
       "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
       "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
       "4  LP001008   Male      No          0      Graduate            No   \n",
       "\n",
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5849                0.0         NaN             360.0   \n",
       "1             4583             1508.0       128.0             360.0   \n",
       "2             3000                0.0        66.0             360.0   \n",
       "3             2583             2358.0       120.0             360.0   \n",
       "4             6000                0.0       141.0             360.0   \n",
       "\n",
       "   Credit_History Property_Area Loan_Status  \n",
       "0             1.0         Urban           Y  \n",
       "1             1.0         Rural           N  \n",
       "2             1.0         Urban           Y  \n",
       "3             1.0         Urban           Y  \n",
       "4             1.0         Urban           Y  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us view the first 5 rows/records of the data\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us heck the data types of the predictors. Note that the columns of type object refers to categorical predictors while others of type int 64 and float 64 are numerical predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loan_ID               object\n",
       "Gender                object\n",
       "Married               object\n",
       "Dependents            object\n",
       "Education             object\n",
       "Self_Employed         object\n",
       "ApplicantIncome        int64\n",
       "CoapplicantIncome    float64\n",
       "LoanAmount           float64\n",
       "Loan_Amount_Term     float64\n",
       "Credit_History       float64\n",
       "Property_Area         object\n",
       "Loan_Status           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class distribution\n",
    "\n",
    "The target/dependent variable is Loan_Status. its distribution is plotted below as piechart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6373d29710>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Loan_Status'].value_counts().plot.pie(y='Loan_Status',figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows that the number of customers with rejected application is much less (less than a half) than the number of custormers with  granted loan, which means that the data is imbalanced.\n",
    "What about making the data balanced by duplicating the minority class examples i.e, using random oversampling? Shall we upsample our data now? \n",
    "\n",
    "No because firstly, we need to clean the data and secondly, upsampling the data before splitting it into training and validation sets would cause data leakage (see SMOTE() below)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loan_ID               0\n",
       "Gender               13\n",
       "Married               3\n",
       "Dependents           15\n",
       "Education             0\n",
       "Self_Employed        32\n",
       "ApplicantIncome       0\n",
       "CoapplicantIncome     0\n",
       "LoanAmount           22\n",
       "Loan_Amount_Term     14\n",
       "Credit_History       50\n",
       "Property_Area         0\n",
       "Loan_Status           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting missing values for each columns\n",
    "df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The total number of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum(axis=0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There several methods which are commonly used to handle missing values in a dataset. At the moment, we will use the simplest one which removes the records/rows with null values from the data. Other methods will be presented and used in the next notebooks.\n",
    "\n",
    "Can we drop rows with missing values before splitting dataset into training and validation folds? \n",
    "\n",
    "Yes, we can as this would not lead to data leakage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows, after rows with missing values are removed, is equal to 480 which is greater than (614-149). This is because some rows have more than one missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the 'Loan_ID' column\n",
    "The loan_Id predictor is uninformative predictor so it will removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop('Loan_ID', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe of indepoendent variables/predictors can be created by removing the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('Loan_Status', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all numerical predictors into float43\n",
    "def convert_float64(x):\n",
    "    return x.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Feature Preprocessing \n",
    " \n",
    "Some numerical features with a large scale may have a sort of domination during the stage of model training, which has an impact on the optiml model's parameters in the end. t\n",
    "Therefor, transforming predictoors into a new form can reduce such imapch and lead to a better model performance. \n",
    "\n",
    "Preprocessing functions are predictor-datatype dependent. For instance, methods applied to numerical predictors are differemt from those used for categorical features. Take a numerical predictor for example, StandardScaler()makes it zero-centred and scaled to unit variance.   \n",
    "\n",
    "Can we apply precessing functions to the whole data i.e before splitting? \n",
    "No, It should be implemented after data is splitted into training and validation folds to prevent data leaskage. \n",
    "\n",
    "What would happen if we preprocessed the whole data? \n",
    "\n",
    "If StandardScaler() is used, the mean and standard deviation should be computed first from the whole data. When data is splitted into training and validation folds, the validation subset would be scaled based on parameters computed using both training and validation data. As a result, data leakage occurs.\n",
    "\n",
    "\n",
    "To tackle such a problem, the preprocessor should be built based only on training data and used to transform the validation set during cross-validation. In this tutorial,  ColumnTransformer() is used to apply appropriate pipelines to both numerical and categorical predictors/columns of the training folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical columns\n",
    "numerical_cols=X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Numerical p[ipeline\n",
    "numerical_pipeline=make_pipeline(FunctionTransformer(func=convert_float64, validate=False),\n",
    "    StandardScaler())\n",
    "\n",
    "# Categorical columns\n",
    "categorical_cols=X.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "\n",
    "# Categorical pipeline\n",
    "categorical_pipeline = make_pipeline(OneHotEncoder(categories='auto', drop='first'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor1 = ColumnTransformer(\n",
    "    [('numerical_preprocessing', numerical_pipeline, numerical_cols),\n",
    "     ('categorical_preprocessing', categorical_pipeline, categorical_cols)],\n",
    "     remainder='drop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "preprocessor2=SMOTE(sampling_strategy='minority') # It will be only applied to the training folds \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['Loan_Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y=y.replace(['Y','N'],[1,0]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    332\n",
       "0    148\n",
       "Name: Loan_Status, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "def build_model(input_size):\n",
    "    model=Sequential()\n",
    "    \n",
    "    #KI= initializers.RandomNormal(0.0,0.05)\n",
    "    KI= initializers.GlorotNormal()\n",
    "    BI= initializers.zeros()\n",
    "    model.add(Dense(100, activation='relu', name='layer1', kernel_initializer=KI,bias_initializer=BI,input_dim=input_size))\n",
    "    \n",
    "    #model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(200, activation='relu', name='layer2', kernel_initializer=KI,bias_initializer=BI))\n",
    "\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(10, activation='relu', name='layer3', kernel_initializer=KI,bias_initializer=BI))\n",
    "\n",
    "    model.add(Dense(1,name='layer4', kernel_initializer=KI,bias_initializer=BI))\n",
    "    # 1- Categorical Classification: The number of neurons in the output layer is exactly the same as the number of classes (2,..)\n",
    "    # the Softmax function is used to conver logits to probability\n",
    "\n",
    "    #loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # 2- Binary Classification: The output layer has only one neuron \n",
    "    # and the Sigmoid function is used to conver logits to probability\n",
    "    loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True) \n",
    "    model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the target variable of each batch extracted from the training folds is evenly distributed (balanced), BalancedBatchGenerator() is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.keras import BalancedBatchGenerator\n",
    "\n",
    "def fit_predict(X_train, y_train, X_test, y_test):\n",
    "    model = build_model(X_train.shape[1])\n",
    "    training_generator = BalancedBatchGenerator(X_train, y_train,batch_size=32)\n",
    "    model.fit_generator(generator=training_generator, epochs=10, verbose=1)\n",
    "\n",
    "    #model.fit(X_train, y_train, epochs=25, verbose=1, batch_size=32)\n",
    "    y_pred = model.predict_proba(X_test)\n",
    "    return roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stratified folds are made by preserving the percentage of samples for each class as shopwn below. Bothe training and validation data are balaned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,  shuffle=True)\n",
    "\n",
    "class_dist_train=[]\n",
    "class_dist_test=[]\n",
    "\n",
    "for train_idx, valid_idx in skf.split(X, y):\n",
    "    X_local_train = X.iloc[train_idx]\n",
    "    y_local_train = y.iloc[train_idx]\n",
    "        \n",
    "    X_local_test =X.iloc[valid_idx]\n",
    "    y_local_test = y.iloc[valid_idx]\n",
    "    class_dist_train.append(y_local_train.value_counts().tolist())\n",
    "    class_dist_test.append(y_local_test.value_counts().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[266, 118], [266, 118], [266, 118], [265, 119], [265, 119]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dist_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[66, 30], [66, 30], [66, 30], [67, 29], [67, 29]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dist_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Accuracy(X,y, upsample=True):\n",
    "    Accuracy=[]\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    for train_idx, valid_idx in skf.split(X, y):\n",
    "        X_local_train = preprocessor1.fit_transform(X.iloc[train_idx])\n",
    "        y_local_train = y.iloc[train_idx].values.ravel()\n",
    "        if upsample:\n",
    "            X_local_train ,  y_local_train =preprocessor2.fit_sample( X_local_train ,  y_local_train)\n",
    "       \n",
    "        X_local_test = preprocessor1.transform(X.iloc[valid_idx])\n",
    "        y_local_test = y.iloc[valid_idx].values.ravel()\n",
    "        \n",
    "                                                \n",
    "        roc_auc = fit_predict(X_local_train, y_local_train, X_local_test, y_local_test)\n",
    "        Accuracy.append(roc_auc)\n",
    "    return Accuracy\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-24-e39e641ea5bb>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6679 - accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5840 - accuracy: 0.5156\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5203 - accuracy: 0.6719\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4921 - accuracy: 0.7539\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4765 - accuracy: 0.7383\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4608 - accuracy: 0.7656\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4464 - accuracy: 0.7695\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4401 - accuracy: 0.7773\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4247 - accuracy: 0.7734\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4168 - accuracy: 0.7891\n",
      "WARNING:tensorflow:From <ipython-input-24-e39e641ea5bb>:10: Sequential.predict_proba (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use `model.predict()` instead.\n",
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6685 - accuracy: 0.5117\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5984 - accuracy: 0.5117\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5580 - accuracy: 0.5195\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5323 - accuracy: 0.6641\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.7305\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4959 - accuracy: 0.7148\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4825 - accuracy: 0.7383\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4755 - accuracy: 0.7383\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4630 - accuracy: 0.7773\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4575 - accuracy: 0.7852\n",
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6677 - accuracy: 0.4961\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5987 - accuracy: 0.5039\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5472 - accuracy: 0.5508\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5194 - accuracy: 0.7305\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5037 - accuracy: 0.7305\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.7305\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4843 - accuracy: 0.7266\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4714 - accuracy: 0.7305\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4592 - accuracy: 0.7344\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4587 - accuracy: 0.7539\n",
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6587 - accuracy: 0.5078\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5708 - accuracy: 0.5234\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5168 - accuracy: 0.6758\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4905 - accuracy: 0.7305\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4681 - accuracy: 0.7500\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4538 - accuracy: 0.7344\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4351 - accuracy: 0.7734\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4286 - accuracy: 0.7773\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4136 - accuracy: 0.7656\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3993 - accuracy: 0.7812\n",
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6714 - accuracy: 0.4922\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6001 - accuracy: 0.4961\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5412 - accuracy: 0.6211\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4973 - accuracy: 0.7227\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4742 - accuracy: 0.7383\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4589 - accuracy: 0.7617\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4455 - accuracy: 0.7539\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4364 - accuracy: 0.7773\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4202 - accuracy: 0.7773\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4079 - accuracy: 0.7695\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f632428f4d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6855 - accuracy: 0.4844\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6561 - accuracy: 0.6133\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6398 - accuracy: 0.6836\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6289 - accuracy: 0.7266\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6168 - accuracy: 0.7500\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6078 - accuracy: 0.7539\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5965 - accuracy: 0.7539\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5861 - accuracy: 0.7773\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5719 - accuracy: 0.7695\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5577 - accuracy: 0.7891\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f632417c3b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6731 - accuracy: 0.5039\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6268 - accuracy: 0.5039\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6002 - accuracy: 0.5039\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5832 - accuracy: 0.5039\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5747 - accuracy: 0.5039\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5697 - accuracy: 0.5039\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5647 - accuracy: 0.5039\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5560 - accuracy: 0.5039\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5416 - accuracy: 0.5039\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5213 - accuracy: 0.5117\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6324073d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6290 - accuracy: 0.4961\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5445 - accuracy: 0.5039\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5011 - accuracy: 0.6719\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4762 - accuracy: 0.7266\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4648 - accuracy: 0.7305\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4540 - accuracy: 0.7539\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4377 - accuracy: 0.7773\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4276 - accuracy: 0.7539\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4190 - accuracy: 0.7812\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4063 - accuracy: 0.7695\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f63240edd40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6382 - accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5752 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5485 - accuracy: 0.5039\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5256 - accuracy: 0.6445\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5129 - accuracy: 0.7148\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4979 - accuracy: 0.7344\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4852 - accuracy: 0.7617\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4770 - accuracy: 0.7773\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4668 - accuracy: 0.7773\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4580 - accuracy: 0.7734\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6324303f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6695 - accuracy: 0.5156\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5849 - accuracy: 0.5977\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5328 - accuracy: 0.7188\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5002 - accuracy: 0.7148\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4750 - accuracy: 0.7539\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4576 - accuracy: 0.7656\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4415 - accuracy: 0.7852\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4315 - accuracy: 0.8086\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4159 - accuracy: 0.7969\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4037 - accuracy: 0.8320\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6324479a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6180 - accuracy: 0.5295\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5408 - accuracy: 0.6684\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5041 - accuracy: 0.7118\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4863 - accuracy: 0.7309\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4722 - accuracy: 0.7361\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4532 - accuracy: 0.7604\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4350 - accuracy: 0.7778\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4184 - accuracy: 0.8056\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.3979 - accuracy: 0.8160\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.3815 - accuracy: 0.8229\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f632464d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6221 - accuracy: 0.5573\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5319 - accuracy: 0.7083\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.6962\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.7222\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4732 - accuracy: 0.7674\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4598 - accuracy: 0.7604\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4506 - accuracy: 0.7917\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4329 - accuracy: 0.7795\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4272 - accuracy: 0.7882\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4126 - accuracy: 0.8021\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6340098170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6026 - accuracy: 0.5191\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5214 - accuracy: 0.7153\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4903 - accuracy: 0.7361\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4730 - accuracy: 0.7465\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4561 - accuracy: 0.7378\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4410 - accuracy: 0.7535\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4338 - accuracy: 0.7674\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4157 - accuracy: 0.7639\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4155 - accuracy: 0.7639\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4066 - accuracy: 0.7639\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f63240739e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6520 - accuracy: 0.5191\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5425 - accuracy: 0.7344\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4811 - accuracy: 0.7396\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4555 - accuracy: 0.7552\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4384 - accuracy: 0.7674\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4300 - accuracy: 0.7622\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4164 - accuracy: 0.7622\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4003 - accuracy: 0.8021\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.3923 - accuracy: 0.7969\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.3792 - accuracy: 0.8212\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f63085fad40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6188 - accuracy: 0.4913\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5420 - accuracy: 0.5903\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5090 - accuracy: 0.7257\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4862 - accuracy: 0.7413\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4724 - accuracy: 0.7431\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4550 - accuracy: 0.7604\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4492 - accuracy: 0.7517\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4314 - accuracy: 0.7865\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4213 - accuracy: 0.7865\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4062 - accuracy: 0.7812\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f63084eab00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6303 - accuracy: 0.5295\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5270 - accuracy: 0.7344\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4912 - accuracy: 0.7431\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4714 - accuracy: 0.7604\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4615 - accuracy: 0.7708\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4522 - accuracy: 0.7656\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4326 - accuracy: 0.7899\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4260 - accuracy: 0.7899\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4131 - accuracy: 0.8003\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.3968 - accuracy: 0.8108\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f630830b560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6097 - accuracy: 0.5104\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5096 - accuracy: 0.6753\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4765 - accuracy: 0.7639\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4510 - accuracy: 0.7691\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4313 - accuracy: 0.7951\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4139 - accuracy: 0.7899\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.3947 - accuracy: 0.8177\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.3899 - accuracy: 0.7951\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.3906 - accuracy: 0.8056\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.3629 - accuracy: 0.8524\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6324049b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6271 - accuracy: 0.5434\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5122 - accuracy: 0.7326\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4825 - accuracy: 0.7656\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4654 - accuracy: 0.7500\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4498 - accuracy: 0.7552\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4386 - accuracy: 0.7760\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4233 - accuracy: 0.7795\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4178 - accuracy: 0.7812\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4047 - accuracy: 0.7986\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8160\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f63084ea440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6254 - accuracy: 0.5503\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5070 - accuracy: 0.7396\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4765 - accuracy: 0.7483\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4630 - accuracy: 0.7674\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4449 - accuracy: 0.7674\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4269 - accuracy: 0.7934\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4115 - accuracy: 0.8056\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.3948 - accuracy: 0.8229\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.3838 - accuracy: 0.8368\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.3714 - accuracy: 0.8368\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f63245a0f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6470 - accuracy: 0.5017\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5612 - accuracy: 0.6128\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5300 - accuracy: 0.7083\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5094 - accuracy: 0.7274\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4945 - accuracy: 0.7396\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4769 - accuracy: 0.7431\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4617 - accuracy: 0.7708\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4566 - accuracy: 0.7691\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4339 - accuracy: 0.7882\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4238 - accuracy: 0.7778\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6324774050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n"
     ]
    }
   ],
   "source": [
    "# Model training and cross validation\n",
    "\n",
    "Accuracy_nosample=Accuracy(X,y, False) # Original data\n",
    "Accuracy_upsample=Accuracy(X,y,True)   # with oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'Accyracy_nosample': Accuracy_nosample  , 'Accuracy_upsample':Accuracy_upsample}\n",
    "df_results=pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accyracy_nosample</th>\n",
       "      <th>Accuracy_upsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.787879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.751515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.810101</td>\n",
       "      <td>0.725253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.771717</td>\n",
       "      <td>0.670707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.795960</td>\n",
       "      <td>0.864646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.749495</td>\n",
       "      <td>0.692929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.761616</td>\n",
       "      <td>0.715152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.753535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.781513</td>\n",
       "      <td>0.640756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.638655</td>\n",
       "      <td>0.852941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accyracy_nosample  Accuracy_upsample\n",
       "0           0.696970           0.787879\n",
       "1           0.868687           0.751515\n",
       "2           0.810101           0.725253\n",
       "3           0.771717           0.670707\n",
       "4           0.795960           0.864646\n",
       "5           0.749495           0.692929\n",
       "6           0.761616           0.715152\n",
       "7           0.747475           0.753535\n",
       "8           0.781513           0.640756\n",
       "9           0.638655           0.852941"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f630869e550>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_results.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart of validation accuracy shows that upsampling the training data has not improved the model's performance. This can be explained as a result of duplicating examples of the minority class in the training data, which made the model learn/overfit the training data well. This led to large generalization error when the model was evaluated on the unseen \n",
    "validation data. \n",
    "\n",
    "But wait a moment! There is something else we need to blame here for such results, which is the data size. It became even smaller after dropping rows with missing values which considerably increased the overfitting after oversampling had been applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
